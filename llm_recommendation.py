# -*- coding: utf-8 -*-
"""LLM_recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18uiiupii0KAOd4Rsy4mhMU641-XjlbX4
"""

import ast
import json
import pandas as pd

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForCausalLM,  BitsAndBytesConfig
from huggingface_hub import login
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import transformers
import torch
from langchain_community.llms import HuggingFacePipeline
from langchain.memory import ConversationBufferMemory
import gradio as gr
from langchain.prompts import PromptTemplate
from transformers import pipeline
import accelerate




def robust_parse(s):
    try:
        return json.loads(s)
    except json.JSONDecodeError:
        try:
            return ast.literal_eval(s)
        except Exception:
            return []

df_movies = pd.read_csv('tmdb_5000_movies.csv')
df_credits = pd.read_csv('tmdb_5000_credits.csv')

# Merge on title
df_merged = df_movies.merge(df_credits, on='title')

# Extract genres
df_merged['Genres'] = df_merged['genres'].apply(lambda x: ", ".join([i['name'] for i in robust_parse(x)]))

# Extract director
def get_director(crew_str):
    crew = robust_parse(crew_str)
    for member in crew:
        if member.get('job') == 'Director':
            return member.get('name', '')
    return ''

df_merged['Director'] = df_merged['crew'].apply(get_director)

# Extract cast (top 5)
def get_cast(cast_str):
    cast_list = robust_parse(cast_str)
    return ", ".join([actor['name'] for actor in cast_list[:5]])

df_merged['Cast'] = df_merged['cast'].apply(get_cast)

# Build combined_info in requested format
df_merged['combined_info'] = df_merged.apply(
    lambda row: f"Type: Movie, Title: {row['title']}, Director: {row['Director']}, Cast: {row['Cast']}, Released: {row['release_date']}, Genres: {row['Genres']}, Vote_Average: {row['vote_average']}, Description: {row['overview']}",
    axis=1
)

# Select relevant final columns
df_final_cleaned = df_merged[['title', 'release_date', 'overview', 'Genres', 'Cast', 'Director', 'combined_info']]

# Display few samples to verify
df_final_cleaned.head()

df_merged[['combined_info']].to_csv('updated_movies.csv',index=False)


loader=CSVLoader(file_path='updated_movies.csv', encoding='utf-8')
data=loader.load()
data

text_splitter=CharacterTextSplitter(chunk_size=1000,chunk_overlap=30, separator="\n")
docs = text_splitter.split_documents(documents=data)


# Load embedding model
embedding_model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": "cuda"}
embeddings = HuggingFaceEmbeddings(
    model_name=embedding_model_name,
    model_kwargs=model_kwargs
)

vectorstore = FAISS.from_documents(docs, embeddings)

# Save and reload the vector store
vectorstore.save_local("faiss_index_")
persisted_vectorstore = FAISS.load_local("faiss_index_", embeddings, allow_dangerous_deserialization=True)
retriever = persisted_vectorstore.as_retriever(search_kwargs={"k":3})


login("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")
model_id = "google/gemma-3-4b-it"

hf_pipeline = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto")

llm = HuggingFacePipeline(pipeline=hf_pipeline)


template_prefix="""You are an expert movie recommender. For user queries about actors/directors/genres:
1. Suggest exactly 3 SPECIFIC movies with YEAR and LEAD ACTORS
2. Include 1 to 3-sentence descriptions
3. Explain WHY they match the request
4. NEVER suggest irrelevant movies


If the user asks for details about any movie or about the cast, provide the following:
1. The movie's full description, including the plot.
2. Information on the lead actors and their roles.
3. Special details about the movie like notable achievements, awards, or critical reception.

Example good response:
"Here are great Russell Crowe movies:
- Gladiator (2000): A former Roman general seeks revenge on the corrupt emperor who murdered his family and sentenced him to slavery. Features Crowe's iconic performance.
- A Beautiful Mind (2001): A Beautiful Mind is a 2001 American biographical drama film about the mathematician John Nash, a Nobel Laureate in Economics, played by Russell Crowe. Crowe won an Oscar for this role.
Why recommended? All showcase Crowe's range in historical dramas and character-driven stories."

IMPORTANT:  If the user asks for "all movies," "every movie," or something similar, DO NOT try to list every movie.
Instead, suggest a few popular movies from different genres.
Explain that listing all movies is not possible.
Do not add " or \h1 at the end of the response

Context: {context}"""

user_info = """This is what we know about the user, and you can use this information to better tune your research:
Age: {age}
Gender: {gender}"""

chat_history_part = """Chat History:
{chat_history}"""

template_suffix= """Question: {question}
Your response:"""

user_info=user_info.format(age=18, gender='male')

COMBINED_PROMPT = template_prefix +'\n'+ user_info +'\n'+ chat_history_part + "\n\n" + template_suffix


PROMPT=PromptTemplate(template=COMBINED_PROMPT, input_variables=["context", "age", "gender", "chat_history", "question"])
chain_type_kwargs = {"prompt": PROMPT, "memory":ConversationBufferMemory(memory_key="chat_history", input_key="question")}
#memory = ConversationBufferMemory(memory_key="chat_history", output_key="result")
qa = RetrievalQA.from_chain_type(llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=False,
    chain_type_kwargs=chain_type_kwargs)

import os

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
'''while True:
    query = input("Type your query (or type 'Exit' to quit): \n")
    if query.lower() == "exit":
        break
    result = qa.invoke(query)
    response=result["result"]
    if "Your response:" in  response:
         response =  response.split("Your response:")[-1].strip()

    print(response)'''

def handle_conversation(message,history):
    result= qa({"query":message})
    response= result["result"]
    if "Your response:" in  response:
         response =  response.split("Your response:")[-1].strip()
    return response


demo= gr.ChatInterface(fn=handle_conversation, title="Movie Blasters", description="Your AI-powered movie recommendation assistant",chatbot=gr.Chatbot(
        value=[],  # Start with empty history
        type="messages",
        height="calc(100vh - 200px)",  # Dynamic height (viewport minus header/footer)
        container=True,  # Allow chatbox to expand within its container
    ),
    theme=gr.themes.Soft(),

)
demo.launch(server_name="0.0.0.0", server_port=7862)

